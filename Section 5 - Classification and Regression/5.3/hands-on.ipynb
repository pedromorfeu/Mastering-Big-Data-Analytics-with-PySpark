{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# Load and parse the data file, converting it to a DataFrame.\n",
    "sample_libsvm_data = spark.read.format(\"libsvm\").load(\n",
    "    \"/usr/local/spark-2.4.3-bin-hadoop2.7/data/mllib/sample_libsvm_data.txt\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+\n",
      "|label|            features|\n",
      "+-----+--------------------+\n",
      "|  0.0|(692,[127,128,129...|\n",
      "|  1.0|(692,[158,159,160...|\n",
      "|  1.0|(692,[124,125,126...|\n",
      "|  1.0|(692,[152,153,154...|\n",
      "|  1.0|(692,[151,152,153...|\n",
      "|  0.0|(692,[129,130,131...|\n",
      "|  1.0|(692,[158,159,160...|\n",
      "|  1.0|(692,[99,100,101,...|\n",
      "|  0.0|(692,[154,155,156...|\n",
      "|  0.0|(692,[127,128,129...|\n",
      "|  1.0|(692,[154,155,156...|\n",
      "|  0.0|(692,[153,154,155...|\n",
      "|  0.0|(692,[151,152,153...|\n",
      "|  1.0|(692,[129,130,131...|\n",
      "|  0.0|(692,[154,155,156...|\n",
      "|  1.0|(692,[150,151,152...|\n",
      "|  0.0|(692,[124,125,126...|\n",
      "|  0.0|(692,[152,153,154...|\n",
      "|  1.0|(692,[97,98,99,12...|\n",
      "|  1.0|(692,[124,125,126...|\n",
      "+-----+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "from pyspark.ml.feature import StringIndexer, VectorIndexer\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "sample_libsvm_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.ml.classification.DecisionTreeClassifier'>\n"
     ]
    }
   ],
   "source": [
    "# Creating our stages:\n",
    "\n",
    "# STAGE 1:\n",
    "# Index labels, adding metadata to the label column.\n",
    "# Fit on whole dataset to include all labels in index.\n",
    "label_indexer = StringIndexer(inputCol=\"label\", outputCol=\"indexedLabel\").fit(\n",
    "    sample_libsvm_data\n",
    ")\n",
    "\n",
    "# STAGE 2:\n",
    "# Automatically identify categorical features, and index them.\n",
    "# We specify maxCategories so features with > 4 distinct values are treated as continuous.\n",
    "feature_indexer = VectorIndexer(\n",
    "    inputCol=\"features\", outputCol=\"indexedFeatures\", maxCategories=4\n",
    ").fit(sample_libsvm_data)\n",
    "\n",
    "# STAGE 3:\n",
    "# Train a DecisionTree model.\n",
    "decission_tree_classifier_model = DecisionTreeClassifier(\n",
    "    labelCol=\"indexedLabel\", featuresCol=\"indexedFeatures\"\n",
    ")\n",
    "\n",
    "print(type(decission_tree_classifier_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating our Pipeline:\n",
    "\n",
    "# Chain indexers and tree in a Pipeline\n",
    "pipeline = Pipeline(\n",
    "    stages=[\n",
    "        label_indexer,  # STAGE 1\n",
    "        feature_indexer,  # STAGE 2\n",
    "        decission_tree_classifier_model,  # STAGE 3\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+--------------------+\n",
      "|prediction|indexedLabel|            features|\n",
      "+----------+------------+--------------------+\n",
      "|       1.0|         1.0|(692,[95,96,97,12...|\n",
      "|       1.0|         1.0|(692,[121,122,123...|\n",
      "|       1.0|         1.0|(692,[124,125,126...|\n",
      "|       1.0|         1.0|(692,[126,127,128...|\n",
      "|       1.0|         1.0|(692,[151,152,153...|\n",
      "+----------+------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Test Error = 0.03846 \n"
     ]
    }
   ],
   "source": [
    "# Split the data into training and test sets (30% held out for testing)\n",
    "(training_data, test_data) = sample_libsvm_data.randomSplit([0.7, 0.3])\n",
    "\n",
    "# Train model.  This also runs the indexers.\n",
    "model = pipeline.fit(training_data)\n",
    "\n",
    "# Make predictions.\n",
    "predictions = model.transform(test_data)\n",
    "\n",
    "# Select example rows to display.\n",
    "predictions.select(\"prediction\", \"indexedLabel\", \"features\").show(5)\n",
    "\n",
    "# Select (prediction, true label) and compute test error\n",
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"indexedLabel\", predictionCol=\"prediction\", metricName=\"accuracy\"\n",
    ")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(f\"Test Error = {1.0 - accuracy:.5f} \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[StringIndexer_0edb550832d5, VectorIndexer_e6e2c8d12994, DecisionTreeClassifier_0377e9bfaae3]\n",
      "[StringIndexer_0edb550832d5, VectorIndexer_e6e2c8d12994, DecisionTreeClassificationModel (uid=DecisionTreeClassifier_0377e9bfaae3) of depth 1 with 3 nodes]\n"
     ]
    }
   ],
   "source": [
    "# You can see that the Pipeline and the PipelineModel have the same stages\n",
    "print(pipeline.getStages())\n",
    "print(model.stages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+\n",
      "|label|            features|\n",
      "+-----+--------------------+\n",
      "|  0.0|(692,[127,128,129...|\n",
      "|  1.0|(692,[158,159,160...|\n",
      "|  1.0|(692,[124,125,126...|\n",
      "|  1.0|(692,[152,153,154...|\n",
      "|  1.0|(692,[151,152,153...|\n",
      "|  0.0|(692,[129,130,131...|\n",
      "|  1.0|(692,[158,159,160...|\n",
      "|  1.0|(692,[99,100,101,...|\n",
      "|  0.0|(692,[154,155,156...|\n",
      "|  0.0|(692,[127,128,129...|\n",
      "|  1.0|(692,[154,155,156...|\n",
      "|  0.0|(692,[153,154,155...|\n",
      "|  0.0|(692,[151,152,153...|\n",
      "|  1.0|(692,[129,130,131...|\n",
      "|  0.0|(692,[154,155,156...|\n",
      "|  1.0|(692,[150,151,152...|\n",
      "|  0.0|(692,[124,125,126...|\n",
      "|  0.0|(692,[152,153,154...|\n",
      "|  1.0|(692,[97,98,99,12...|\n",
      "|  1.0|(692,[124,125,126...|\n",
      "+-----+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml.feature import VectorIndexer\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "sample_libsvm_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating our stages:\n",
    "\n",
    "# STAGE 1:\n",
    "# Automatically identify categorical features, and index them.\n",
    "feature_indexer = VectorIndexer(\n",
    "    inputCol=\"features\",\n",
    "    outputCol=\"indexedFeatures\",\n",
    "    # Set maxCategories so features with > 4 distinct values are treated as continuous.\n",
    "    maxCategories=4,\n",
    ").fit(sample_libsvm_data)\n",
    "\n",
    "# STAGE 2:\n",
    "# Train a RandomForest model.\n",
    "random_forest_model = RandomForestRegressor(featuresCol=\"indexedFeatures\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating our Pipeline:\n",
    "# Chain indexer and forest in a Pipeline\n",
    "pipeline = Pipeline(stages=[feature_indexer, random_forest_model])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+--------------------+\n",
      "|prediction|label|            features|\n",
      "+----------+-----+--------------------+\n",
      "|       0.0|  0.0|(692,[121,122,123...|\n",
      "|       0.0|  0.0|(692,[123,124,125...|\n",
      "|       0.0|  0.0|(692,[123,124,125...|\n",
      "|       0.0|  0.0|(692,[124,125,126...|\n",
      "|      0.05|  0.0|(692,[124,125,126...|\n",
      "+----------+-----+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Root Mean Squared Error (RMSE) on test data = 0.143084\n"
     ]
    }
   ],
   "source": [
    "# Split the data into training and test sets (30% held out for testing)\n",
    "(training_data, test_data) = sample_libsvm_data.randomSplit([0.7, 0.3])\n",
    "\n",
    "# Train model.  This also runs the indexer.\n",
    "model = pipeline.fit(training_data)\n",
    "\n",
    "# Make predictions.\n",
    "predictions = model.transform(test_data)\n",
    "\n",
    "# Select example rows to display.\n",
    "predictions.select(\"prediction\", \"label\", \"features\").show(5)\n",
    "\n",
    "# Select (prediction, true label) and compute test error\n",
    "evaluator = RegressionEvaluator(\n",
    "    labelCol=\"label\", predictionCol=\"prediction\", metricName=\"rmse\"\n",
    ")\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(\"Root Mean Squared Error (RMSE) on test data = %g\" % rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VectorIndexer_807bb456cf66, RandomForestRegressor_97a60d492d9e]\n",
      "[VectorIndexer_807bb456cf66, RandomForestRegressionModel (uid=RandomForestRegressor_97a60d492d9e) with 20 trees]\n"
     ]
    }
   ],
   "source": [
    "# You can see that the Pipeline and the PipelineModel have the same stages\n",
    "print(pipeline.getStages())\n",
    "print(model.stages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestRegressionModel (uid=RandomForestRegressor_0e684680411c) with 20 trees\n"
     ]
    }
   ],
   "source": [
    "# The last stage in a PipelineModel is usually the most informative\n",
    "print(model.stages[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pipeline: <class 'pyspark.ml.pipeline.Pipeline'>\n",
      "model: <class 'pyspark.ml.pipeline.PipelineModel'>\n"
     ]
    }
   ],
   "source": [
    "# Here you can see that pipeline and model are Pipeline and PipelineModel classes\n",
    "print(\"pipeline:\", type(pipeline))\n",
    "print(\"model:\", type(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
